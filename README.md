# OpenNMTNTF
This is a baseline project for Neural Team Formation with OpenNMT

# Project Setup
```
git clone https://github.com/jamil2388/OpenNMTNTF.git
cd OpenNMTNTF
pip install -r requirements.txt
```
OpenNMT by default installs _pytorch_ _cpu_ version. In case of gpu usage, install the appropriate torch by running <br>
```
pip uninstall torch
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # sample command for cuda 11.8 compatible GPU
```

# Notes on Steps

### Sparse Matrix Generation using OpeNTF

- If you want to run the translation on toy dataset, you can skip this section
- The _toy.dblp.v12.json_ data and its preprocessed outputs are already given in the required folders
- This project requires the data to be in sparse matrix format, specifically formulated by the OpeNTF project at "https://github.com/fani-lab/OpeNTF". 
Conversion of raw domain data (example : DBLP) to sparse matrix enables in faster processing of the data into corpus. It drastically compresses the raw data size.
This enables us to perform multiple runs on various NMT pipeline settings with maximum reduction of data preprocessing time 
- For instance : The complex DBLP data is compressed into a sparse matrix holding three dictionaries 1) 'id' representing the team of publication,
2) 'skill' representing the keywords (skills) used in the publications and 3) 'member' representing the authors of the publication
- Each row in the sparse matrix holds the one-hot encoded information of individual teams (in this example : publications)
- For example : Two rows of the matrix named _teamsvecs_ generated by OpeNTF will contain something like this :

| id  | skill       | member          |
|-----| ------------- | ----------------- |
| 0   | 0 0 1 0 1 0 | 0 0 0 1 1 0 0 1 |
| 1   | 0 1 0 0 1 1 | 1 0 0 0 0 1 0 0 |

- This will simply give us all the information required to convert the dataset into a translation corpus
- Follow the _Setup_ instructions in the _OpeNTF > README.md_ section
- After setup, let us assume that we have the root OpeNTF project folder deployed as _OpeNTF_  
- Copy the raw domain data file (DBLP file) into _OpeNTF/data/raw/dblp/<data_name>_ folder (Create the folder if it does not exist)
- Remove the files _index.pkl, teams.pkl and teamsvecs.pkl_ (if exists) from the similar folder in _OpeNTF/data/preprocessed/dblp/<data_name>_ folder
- In _OpeNTF/src/param.py_, edit the _'cmd'_ parameter to _'['eval']'_ (This will only run the entire model in evaluation mode (which is empty), just to generate the sparse_matrix)
- Now run the commands : 
```
cd OpeNTF/src
python -u main.py -data ../data/raw/dblp/<data_name> -domain dblp -model fnn
```
- After completion, copy the files _index.pkl, teams.pkl and teamsvecs.pkl_ 
from the folder _OpeNTF/data/preprocessed/dblp/<data_name>_ to _OpenNMTNTF/data/raw/dblp/<data_name>_ folder (Create one if it does not exist)

### Opennmt-py Test Run

- Prepare the _source_ and _target_ data for translation in the data folder
- Prepare _config.yaml_ file for run configurations
- In case of GPU usage, edit the config file and enable the options "_world_size_" and "_gpu_ranks_" as the total number of gpus and id of the gpu device to use respectively
- Build the vocabulary against the source and target train files (example.vocab.src, example.vocab.tgt)
- ```onmt_build_vocab -config toy_en_de.yaml -n_sample 10``` creates the vocab files by only taking _10_ rows (10 samples) each from the train files
- The vocabulary will be the count of the distinct words within the _selected number of samples_


### Structure of "_data_" folder

- The _data_ folder must contain all forms of data
- _preprocessed_ : contains the preprocessed _src_ and _tgt_ corpus for translation from sparse matrix data from opentf
- _raw_ : contains any form of raw data ready to be preprocessed (example : sparse matrix from dblp data)
- _input_ : the train test valid splits of the preprocessed data to be fed for translation
- _output_ : <br>
  1) the generated vocabulary of the model in _vocabs_ folder, 
  2) the generated models in the _models_ folder and 
  3) the generated translations in the _translation_ folder

### Run Commands for Sample data (dblp) 

```
# This scenario is for dblp/toy.dblp.v12.json data
# preprocess the data
python main.py

# build vocabulary
onmt_build_vocab -config config.yaml -n_sample -1 

# train the data
onmt_train -config config.yaml

# predict the translations
onmt_translate -model data/output/models/dblp/toy.dblp.v12.json/_step_100.pt -src data/input/dblp/toy.dblp.v12.json/src_test.txt -output data/output/translations/toy.dblp.v12.json.pred_100.txt -verbose
```

###### Draft Commands 
```
onmt_translate -model data/output/models/dblp/dblp.v12.json.filtered.mt100.ts5/_step_5.pt -src data/input/dblp/dblp.v12.json.filtered.mt100.ts5/src_test.txt -output data/output/translations/dblp.v12.json.filtered.mt100.ts5.pred_5.txt -verbose
onmt_translate -model data/output/models/dblp/dblp.v12.json.filtered.mt100.ts5/_step_5.pt -src data/input/dblp/dblp.v12.json.filtered.mt100.ts5/src_test.txt -output data/output/translations/dblp.v12.json.filtered.mt100.ts5.pred_5.txt -verbose

# for pre trained embeddings
onmt_train -config config.yaml -src_embeddings data/preprocessed/dblp/dblp.v12.json.filtered.mt100.ts5/src.sg0.d5.w3.txt -tgt_embeddings data/preprocessed/dblp/dblp.v1
2.json.filtered.mt100.ts5/tgt.sg0.d5.w3.txt -embeddings_type word2vec

```